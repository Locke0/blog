<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>Yangyue&#39;s Blog</title>
	<subtitle>Locke&#39;s projects</subtitle>
	
	<link href="https://locke0.github.io/blog/feed/feed.xml" rel="self"/>
	<link href="https://locke0.github.io/blog"/>
	<updated>2024-11-11T19:00:00-05:00</updated>
	<id>https://locke0.github.io/blog/posts</id>
	<author>
		<name>Yangyue (Locke) Wang</name>
		<email></email>
	</author>
	
	<entry>
		<title>Composable Energy Based Models Study Notes</title>
		<link href="https://locke0.github.io/blog/posts/ebm_sn/"/>
		<updated>2024-11-11T19:00:00-05:00</updated>
		<id>https://locke0.github.io/blog/posts/ebm_sn/</id>
		<content type="html">
		  &lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Energy Functions&lt;/li&gt;
&lt;li&gt;Partition Functions&lt;/li&gt;
&lt;li&gt;Gradient Descent and Optimization&lt;/li&gt;
&lt;li&gt;Markov Chain Monte Carlo&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Concept&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Energy Function&lt;/td&gt;
&lt;td&gt;A scalar function that assigns low values to likely configurations and high values to unlikely ones&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Partition Function&lt;/td&gt;
&lt;td&gt;Normalizing constant Z that ensures probability distribution sums/integrates to 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Descent&lt;/td&gt;
&lt;td&gt;Optimization method used to find local minima of energy function during inference&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MCMC&lt;/td&gt;
&lt;td&gt;Sampling method used to approximate expectations and generate samples from model&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Instead of regressing input x to output y, &lt;mark&gt;energy based models&lt;/mark&gt; predict whether a certain pair or configuration of variables fit together. Energy functions assign low energy to likely configurations and high energy to unlikely ones.&lt;/p&gt;
&lt;h2 id=&quot;definitions&quot;&gt;Definitions&lt;/h2&gt;
&lt;p&gt;$&#92;mathcal{F}: &#92;mathcal{X} &#92;times &#92;mathcal{Y} &#92;rightarrow &#92;mathbb{R}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;mathcal{F}(x,y)$: describes the level of dependency between x and y&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;check{y} = &#92;arg&#92;min_y &#92;mathcal{F}(x,y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Energy function (used in inference not training):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It needs to be smooth and differentiable to perform gradient-based method for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SIDE NOTE: Graphical models&#39; energy function decomposes as &lt;em&gt;a sum of energy terms&lt;/em&gt;, each of which accounts a subset of variables.&lt;/p&gt;
&lt;p&gt;EBM with latent variables&lt;/p&gt;
&lt;p&gt;y depends on x as well as an extra variable z (the latent variable).&lt;br&gt;
These latent variables can provide auxiliary information.&lt;/p&gt;
&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Minimize the energy function simulataneously over y and z:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;check{y}, &#92;check{z} = &#92;arg&#92;min_{y,z} E(x,y,z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;which is equivalent to:&lt;/p&gt;
&lt;p&gt;$&#92;Rightarrow F_{&#92;infty}(x,y) = &#92;arg&#92;min_z E(x,y,z)$&lt;/p&gt;
&lt;p&gt;$&#92;Rightarrow F_{&#92;beta}(x,y) = -&#92;frac{1}{&#92;beta} &#92;log &#92;int_z &#92;exp(-&#92;beta E(x,y,z))$&lt;/p&gt;
&lt;p&gt;when $&#92; &#92;beta &#92;rightarrow &#92;infty$, $&#92; &#92;check{y}= &#92;arg&#92;min_y F(x,y)$&lt;/p&gt;
&lt;p&gt;Another advantage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By varying the latent variable over a set, we can make the prediction output $y$ vary over the manifold of possible predictions as well: $F(x,y) = &#92;arg&#92;min_zE(x,y,z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;
&lt;p&gt;Constraints:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P(x) &#92;geq 0 &#92; $ is non-negative&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$&#92;sum_x P(x) = 1 &#92; $ OR $&#92; &#92;int_x P(x) dx = 1 &#92; $ if continuous is normalized&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;applications&quot;&gt;Applications&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Video prediction&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;energy-based-models-(ebms)&quot;&gt;Energy-Based Models (EBMs)&lt;/h2&gt;
&lt;p&gt;Energy-Based Models (EBMs) are a class of machine learning models that learn an energy function E(x) to assign low energy to inputs from the data distribution and high energy to others. Key features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Implicit Generation&lt;/em&gt;&lt;/strong&gt;: Samples are generated implicitly through MCMC sampling from the energy function, rather than explicitly by a generator network&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Compositionality&lt;/em&gt;&lt;/strong&gt;: EBMs can combine multiple energy functions to satisfy multiple constraints or goals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Flexibility&lt;/em&gt;&lt;/strong&gt;: EBMs can represent a wide range of probability distributions and can be applied to various types of data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;composable-energy-landscapes&quot;&gt;Composable Energy Landscapes&lt;/h2&gt;
&lt;p&gt;Composable Energy Landscapes provide a framework for constructing models that generalize by composing multiple energy landscapes. This approach allows for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Zero-Shot Generalization&lt;/strong&gt;: The ability to generalize to new distributions by composing learned energy landscapes without additional training&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Energy Function E_θ(x, y)&lt;/strong&gt;: Assigns low energy to accurate predictions, enabling prediction as a search process on the energy landscape&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;mathematical-foundations&quot;&gt;Mathematical Foundations&lt;/h2&gt;
&lt;h3 id=&quot;energy-function-and-boltzmann-distribution&quot;&gt;Energy Function and Boltzmann Distribution&lt;/h3&gt;
&lt;p&gt;The energy function is central to EBMs, with prediction formulated as finding the label y that minimizes the energy:&lt;/p&gt;
&lt;p&gt;y = argmin_y E_θ(x,y)&lt;/p&gt;
&lt;p&gt;The Boltzmann distribution is used for sampling predictions:&lt;/p&gt;
&lt;p&gt;y ~ p(x,y) ∝ e^(-E_θ(x,y))&lt;/p&gt;
&lt;h3 id=&quot;training-objectives&quot;&gt;Training Objectives&lt;/h3&gt;
&lt;p&gt;The primary training objective for EBMs is the Negative Log-Likelihood (NLL):&lt;/p&gt;
&lt;p&gt;L_NLL(D) = E_z~D[E_θ(z)] + log∫e^(-E_θ(z))dz&lt;/p&gt;
&lt;p&gt;The gradient of the NLL is given by:&lt;/p&gt;
&lt;p&gt;∇_θL_NLL(D) = E_z~D[∇_θE_θ(z)] - E_z~p_θ(z)[∇_θE_θ(z)]&lt;/p&gt;
&lt;h2 id=&quot;training-and-sampling-techniques&quot;&gt;Training and Sampling Techniques&lt;/h2&gt;
&lt;h3 id=&quot;maximum-likelihood-estimation-(mle)&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;
&lt;p&gt;MLE is used to minimize the negative log-likelihood:&lt;/p&gt;
&lt;p&gt;L_NLL(D) = E_z~D[E_θ(z)] - E_z~p_θ(z)[E_θ(z)]&lt;/p&gt;
&lt;h3 id=&quot;langevin-dynamics&quot;&gt;Langevin Dynamics&lt;/h3&gt;
&lt;p&gt;Langevin dynamics is a gradient-based MCMC method used for sampling:&lt;/p&gt;
&lt;p&gt;z_t = z_(t-1) - λ∇_zE_θ(z_(t-1)) + √(2λ)ξ, ξ ~ N(0,1)&lt;/p&gt;
&lt;h3 id=&quot;replay-buffer&quot;&gt;Replay Buffer&lt;/h3&gt;
&lt;p&gt;A replay buffer is used to maintain past samples, improving mixing and sample diversity.&lt;/p&gt;
&lt;h2 id=&quot;compositional-modeling&quot;&gt;Compositional Modeling&lt;/h2&gt;
&lt;h3 id=&quot;logical-and-probability-composition&quot;&gt;Logical and Probability Composition&lt;/h3&gt;
&lt;p&gt;EBMs allow for logical composition of energy landscapes to model complex distributions. Probability distributions can be combined through products, mixtures, and inverses.&lt;/p&gt;
&lt;h3 id=&quot;graphical-and-hierarchical-models&quot;&gt;Graphical and Hierarchical Models&lt;/h3&gt;
&lt;p&gt;EBMs can implement undirected and directed graphical models compositionally. They also enable hierarchical composition, combining language, video, and action models for complex planning tasks.&lt;/p&gt;
&lt;h2 id=&quot;applications-2&quot;&gt;Applications&lt;/h2&gt;
&lt;p&gt;EBMs and composable energy landscapes have diverse applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Scene understanding, image generation and editing, domain adaptation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robotics&lt;/strong&gt;: Planning and constraint satisfaction problems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Foundation Models&lt;/strong&gt;: Vision-question answering and hierarchical planning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;observations-and-experiment-results&quot;&gt;Observations and Experiment Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generalization&lt;/strong&gt;: Compositional models show strong generalization to unseen data distributions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: These models often require fewer parameters and less data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: They allow for the incorporation of new constraints at prediction time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;tutorials-and-practical-tips&quot;&gt;Tutorials and Practical Tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apply regularization to ensure energy landscapes are not too sharp&lt;/li&gt;
&lt;li&gt;Learn sequences of energy landscapes for high-dimensional inputs (annealed energy landscapes)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;future-directions&quot;&gt;Future Directions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inverse Design&lt;/strong&gt;: Apply composable energy landscapes to inverse material design and protein synthesis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Broader Applications&lt;/strong&gt;: Explore applications in sciences and engineering&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Develop techniques to scale energy landscape learning to more complex and high-dimensional data&lt;/li&gt;
&lt;/ul&gt;

			
		</content>
	</entry>
</feed>

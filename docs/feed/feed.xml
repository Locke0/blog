<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>Yangyue&#39;s Blog</title>
	<subtitle>Locke&#39;s projects</subtitle>
	
	<link href="https://locke0.github.io/blog/feed/feed.xml" rel="self"/>
	<link href="https://locke0.github.io/blog"/>
  
	  <updated>2025-08-21T06:26:30Z</updated>
  
	<id>https://locke0.github.io/blog/posts</id>
	<author>
		<name>Yangyue (Locke) Wang</name>
		<email></email>
	</author>
	
	<entry>
		<title>Why Do Computer Use Agents Fail? The Brittle Stack of AI Limitations</title>
		<link href="https://locke0.github.io/blog/posts/why_do_computer_use_agents_fail/"/>
		<updated>2025-08-21T06:26:30Z</updated>
		<id>https://locke0.github.io/blog/posts/why_do_computer_use_agents_fail/</id>
		<content type="html">
		  &lt;h3 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h3&gt;
&lt;p&gt;The promise of computer use agents (CUAs), AI systems that can perceive and act on a computer screen just like a human, is a compelling one. We envision an intelligent assistant that can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Book a flight, hotel, &amp;amp; activities&lt;/li&gt;
&lt;li&gt;Organize a annual budget planning spreadsheet&lt;/li&gt;
&lt;li&gt;Debug robotics CAD design&lt;/li&gt;
&lt;li&gt;Navigate complex game engine interfaces&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://locke0.github.io/img/why_do_computer_use_agents_fail/ui-tars-tutorial-screenshot.png&quot; alt=&quot;Human user following tutorial instructions to select spreadsheet data&quot; /&gt;&lt;br /&gt;
(&lt;em&gt;&lt;a href=&quot;https://seed-tars.com/1.5&quot;&gt;UI-TARS research, 2025&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;Over the past year, we&#39;ve seen remarkable progress, with models like UI-TARS-1.5, OpenCUA, &amp;amp; Claude Sonnet 4, and multi-agent systems like CoACT, GTA-1 w/ o3, JEDI w/ o3, &amp;amp; Agent S2 setting new performance records on benchmarks like OSWorld and ScreenSpot-Pro. Yet, despite these advances, the general-purpose CUA remains elusive.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;1-the-perception-challenge-the-world-is-not-always-what-it-seems&quot;&gt;1. The Perception Challenge: The World Is Not Always What It Seems&lt;/h3&gt;
&lt;p&gt;The first and most fundamental challenge for any CUA is perception—the ability to interpret the visual information on a screen. Unlike a human, an agent doesn&#39;t have an intuitive understanding of a graphical user interface (GUI). It just sees pixels, and its job is to make sense of them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Side-by-side comparison: Human sees &amp;quot;Submit button&amp;quot; vs. AI sees &amp;quot;pixel array with RGB values&amp;quot;]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;11-the-problem-of-tail-classes&quot;&gt;1.1 The Problem of Tail Classes&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; Most agents are trained on common websites and popular applications, creating a distribution imbalance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ &lt;strong&gt;Excel at recognizing:&lt;/strong&gt; Standard Submit buttons, common UI elements&lt;/li&gt;
&lt;li&gt;❌ &lt;strong&gt;Completely lost with:&lt;/strong&gt; Rare or specialized UI components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples of problematic &amp;quot;tail classes&amp;quot;:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom data visualization widgets in scientific apps&lt;/li&gt;
&lt;li&gt;Niche financial charting tools&lt;/li&gt;
&lt;li&gt;Complex, multi-layered data selectors&lt;/li&gt;
&lt;li&gt;Highly-specific industrial control panel elements&lt;/li&gt;
&lt;li&gt;Obscure icons from legacy applications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why it happens:&lt;/strong&gt; The model hasn&#39;t seen enough examples of these elements in various contexts to build a robust mental model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Venn diagram showing &amp;quot;Common UI Elements&amp;quot; vs. &amp;quot;Tail Class Elements&amp;quot; with training data distribution]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;12-small-targets-in-a-big-world&quot;&gt;1.2 Small Targets in a Big World&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt; Modern screens have astonishingly high resolution, making critical elements tiny targets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Critical buttons occupy only a tiny fraction of the screen&lt;/li&gt;
&lt;li&gt;State-of-the-art models often fail on benchmarks like ScreenSpot-Pro&lt;/li&gt;
&lt;li&gt;Vision encoders may be trained at lower resolution&lt;/li&gt;
&lt;li&gt;Elements might be partially occluded by pop-ups or sidebars&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Screenshot of a high-resolution screen with tiny UI elements highlighted and zoomed in]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;13-ambiguous-semantics&quot;&gt;1.3 Ambiguous Semantics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Icons and visual elements are not always universal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Examples of ambiguity:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cloud icon: Does it mean &amp;quot;upload to cloud,&amp;quot; &amp;quot;show weather,&amp;quot; or &amp;quot;link to a cloud document&amp;quot;?&lt;/li&gt;
&lt;li&gt;Lightning bolt: &amp;quot;Cast a spell&amp;quot; in a game vs. &amp;quot;express route&amp;quot; in a transportation app&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Root Cause:&lt;/strong&gt; Absence of clear, unambiguous textual labels leads to critical misinterpretations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Grid of ambiguous icons with multiple possible meanings shown]&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;2-the-reasoning-challenge-from-task-to-plan-to-action&quot;&gt;2. The Reasoning Challenge: From Task to Plan to Action&lt;/h3&gt;
&lt;p&gt;Once an agent can perceive the screen, it must reason about the task and form a plan. This involves connecting high-level instructions to low-level actions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Flowchart: &amp;quot;Find cheapest flight&amp;quot; → &amp;quot;Click search box&amp;quot; → &amp;quot;Type destination&amp;quot; → &amp;quot;Click search button&amp;quot;]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;21-the-hallucinating-planner&quot;&gt;2.1 The Hallucinating Planner&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;What happens:&lt;/strong&gt; The agent generates plausible-sounding plans involving elements that don&#39;t exist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Root Causes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Limited context window prevents retaining full UI mental model&lt;/li&gt;
&lt;li&gt;Agent &amp;quot;remembers&amp;quot; elements from previous screenshots no longer present&lt;/li&gt;
&lt;li&gt;Chain-of-Thought (CoT) becomes disconnected from visual reality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Analogy:&lt;/strong&gt; Like a person with short-term memory loss trying to follow a complex recipe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Timeline showing agent&#39;s memory degradation and hallucination of non-existent UI elements]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;22-the-symbol-grounding-gap&quot;&gt;2.2 The Symbol Grounding Gap&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Agent knows the symbol but not the underlying function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Agent sees &amp;quot;delete&amp;quot; and &amp;quot;remove&amp;quot; buttons but doesn&#39;t understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One is permanent&lt;/li&gt;
&lt;li&gt;One is reversible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Current State:&lt;/strong&gt; Agent&#39;s reasoning is purely correlational, not causal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research Direction:&lt;/strong&gt; Frameworks that explicitly model UI state transitions (e.g., UI-TARS).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [State transition diagram showing before/after states for different button actions]&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;23-task-deviation&quot;&gt;2.3 Task Deviation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What happens:&lt;/strong&gt; Agent gets sidetracked from the original goal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Task:&lt;/strong&gt; &amp;quot;Organize my downloads folder&amp;quot;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ Successfully finds the folder&lt;/li&gt;
&lt;li&gt;❌ Gets sidetracked by &amp;quot;open every PDF&amp;quot;&lt;/li&gt;
&lt;li&gt;❌ Never completes the original task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Modular, hierarchical frameworks like Agent S2 use a &amp;quot;Manager&amp;quot; agent to keep &amp;quot;Worker&amp;quot; agents on track.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Hierarchical task tree showing main goal and sub-goals with deviation paths highlighted]&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;3-the-action-challenge-the-friction-of-the-real-world&quot;&gt;3. The Action Challenge: The Friction of the Real World&lt;/h3&gt;
&lt;p&gt;Even with perfect perception and flawless planning, execution remains challenging in a dynamic, unpredictable environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Diagram showing the gap between planned actions and real-world execution challenges]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;31-the-brittle-action-space&quot;&gt;3.1 The Brittle Action Space&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Current Limitation:&lt;/strong&gt; Most agents are limited to fixed atomic actions (e.g., &lt;code&gt;click(x, y)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&#39;s Missing:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complex drag-and-drop operations&lt;/li&gt;
&lt;li&gt;Multi-finger touch gestures&lt;/li&gt;
&lt;li&gt;Precise slider movements&lt;/li&gt;
&lt;li&gt;Programmatic control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Emerging Solution:&lt;/strong&gt; Agents generating code (e.g., Python scripts) to control the environment directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Comparison table: Current actions vs. needed actions vs. future capabilities]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;32-lack-of-fault-tolerance&quot;&gt;3.2 Lack of Fault Tolerance&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; What happens when things go wrong?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Failure Scenarios:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Website unresponsive&lt;/li&gt;
&lt;li&gt;Element fails to load&lt;/li&gt;
&lt;li&gt;Action doesn&#39;t produce expected result&lt;/li&gt;
&lt;li&gt;HTTP 500 errors&lt;/li&gt;
&lt;li&gt;No response from commands&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Current State:&lt;/strong&gt; Agent gets stuck or repeats failing actions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research Direction:&lt;/strong&gt; Reflection and error-correction loops for robust recovery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Error handling flowchart showing detection → diagnosis → recovery → retry]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;33-memory-and-state-drift&quot;&gt;3.3 Memory and State Drift&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt; Agent&#39;s internal state must stay synchronized with dynamic screen state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Operating on outdated UI information&lt;/li&gt;
&lt;li&gt;Missing animations, pop-ups, or transient changes&lt;/li&gt;
&lt;li&gt;Stale mental model of the interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Analogy:&lt;/strong&gt; Difference between a static photograph and a live video stream.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solutions:&lt;/strong&gt; Sophisticated, persistent memory systems (Zep AI, Mem0).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Timeline showing state drift over time with before/after screenshots]&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;4-the-road-ahead-solutions-and-research-directions&quot;&gt;4. The Road Ahead: Solutions and Research Directions&lt;/h3&gt;
&lt;p&gt;The failures of computer use agents are not fundamental flaws but a clear roadmap for future research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Roadmap diagram showing current state → challenges → solutions → future vision]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;41-toward-end-to-end-models&quot;&gt;4.1 Toward End-to-End Models&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Current Approach:&lt;/strong&gt; Modular, separate components for perception, reasoning, and action.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future Direction:&lt;/strong&gt; Training agents on huge, multi-modal datasets capturing full human workflow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn causal relationships between actions and outcomes&lt;/li&gt;
&lt;li&gt;Reduce symbol grounding gaps&lt;/li&gt;
&lt;li&gt;Integrated learning across all components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Inspiration:&lt;/strong&gt; Success of large foundation models in other domains.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Architecture comparison: Current modular vs. future end-to-end]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;42-better-benchmarks&quot;&gt;4.2 Better Benchmarks&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Current State:&lt;/strong&gt; Simple, canned tasks in controlled environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future Direction:&lt;/strong&gt; Live, interactive benchmarks like BrowseComp and WebArena.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Introduces real-world variability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fluctuating ad placements&lt;/li&gt;
&lt;li&gt;Session timeouts&lt;/li&gt;
&lt;li&gt;Different website versions&lt;/li&gt;
&lt;li&gt;Dynamic content changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Benchmark comparison chart showing complexity levels and real-world factors]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;43-the-power-of-self-improvement&quot;&gt;4.3 The Power of Self-Improvement&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Key Trend:&lt;/strong&gt; Agents learning from their own failures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Techniques:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Online trace bootstrapping&lt;/li&gt;
&lt;li&gt;Direct preference optimization (DPO)&lt;/li&gt;
&lt;li&gt;Agent-in-the-loop processes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Process:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Agent performs task&lt;/li&gt;
&lt;li&gt;Performance evaluated&lt;/li&gt;
&lt;li&gt;Feedback used to correct mistakes&lt;/li&gt;
&lt;li&gt;Reasoning refined for future attempts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Learning loop diagram showing performance → evaluation → correction → improvement]&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;44-beyond-the-pixels&quot;&gt;4.4 Beyond the Pixels&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Current Limitation:&lt;/strong&gt; Agents only look at visual pixels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future Enhancement:&lt;/strong&gt; Leverage multiple information sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTML DOM tree&lt;/li&gt;
&lt;li&gt;Accessibility APIs&lt;/li&gt;
&lt;li&gt;System logs&lt;/li&gt;
&lt;li&gt;Underlying metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; DOM tree tells agent &lt;code&gt;&amp;lt;button id=&amp;quot;submit&amp;quot;&amp;gt;&lt;/code&gt; rather than just visual appearance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Benefit:&lt;/strong&gt; More robust to visual changes and layout shifts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Multi-modal information fusion diagram showing pixels + DOM + APIs + logs]&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h3&gt;
&lt;p&gt;The journey to a truly generalist computer use agent is ongoing. With each new paper and each failed run on a leaderboard, we get:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A clearer picture of the challenges ahead&lt;/li&gt;
&lt;li&gt;A better sense of how to solve them&lt;/li&gt;
&lt;li&gt;Progress toward robust, real-world AI assistants&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Visual Placeholder: [Progress timeline showing key milestones and future goals]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaway:&lt;/strong&gt; The failures we&#39;re seeing today are not setbacks but stepping stones toward more capable, robust computer use agents that can truly understand and navigate the digital world as humans do.&lt;/p&gt;

			
		</content>
	</entry>
</feed>

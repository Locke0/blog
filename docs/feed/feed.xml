<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>Yangyue&#39;s Blog</title>
	<subtitle>Locke&#39;s projects</subtitle>
	
	<link href="https://locke0.github.io/blog/feed/feed.xml" rel="self"/>
	<link href="https://locke0.github.io/blog"/>
	<updated>2024-11-11T19:00:00-05:00</updated>
	<id>https://locke0.github.io/blog/posts</id>
	<author>
		<name>Yangyue (Locke) Wang</name>
		<email></email>
	</author>
	
	<entry>
		<title>Energy Based Models Study Notes</title>
		<link href="https://locke0.github.io/blog/posts/ebm_sn/"/>
		<updated>2024-11-11T19:00:00-05:00</updated>
		<id>https://locke0.github.io/blog/posts/ebm_sn/</id>
		<content type="html">
		  &lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Energy Functions&lt;/li&gt;
&lt;li&gt;Partition Functions&lt;/li&gt;
&lt;li&gt;Gradient Descent and Optimization&lt;/li&gt;
&lt;li&gt;Markov Chain Monte Carlo&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Instead of regressing input x to output y, energy based models predict whether a certain pair of  fit together.&lt;/p&gt;
&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;$&#92;mathcal{F}: &#92;mathcal{X} &#92;times &#92;mathcal{Y} &#92;rightarrow &#92;mathbb{R}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;mathcal{F}(x,y)$: describes the level of dependency between x and y&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;check{y} = &#92;arg&#92;min_y &#92;mathcal{F}(x,y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Energy function (used in inference not training):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It needs to be smooth and differentiable to perform gradient-based method for inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SIDE NOTE: Graphical models&#39; energy function decomposes as &lt;em&gt;a sum of energy terms&lt;/em&gt;, each of which accounts a subset of variables.&lt;/p&gt;
&lt;p&gt;EBM with latent variables&lt;/p&gt;
&lt;p&gt;y depends on x as well as an extra variable z (the latent variable).&lt;br&gt;
These latent variables can provide auxiliary information.&lt;/p&gt;
&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;
&lt;p&gt;Minimize the energy function simulataneously over y and z:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;check{y}, &#92;check{z} = &#92;arg&#92;min_{y,z} E(x,y,z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;which is equivalent to:&lt;/p&gt;
&lt;p&gt;$&#92;Rightarrow F_{&#92;infty}(x,y) = &#92;arg&#92;min_z E(x,y,z)$&lt;/p&gt;
&lt;p&gt;$&#92;Rightarrow F_{&#92;beta}(x,y) = -&#92;frac{1}{&#92;beta} &#92;log &#92;int_z &#92;exp(-&#92;beta E(x,y,z))$&lt;/p&gt;
&lt;p&gt;when $&#92; &#92;beta &#92;rightarrow &#92;infty$, $&#92; &#92;check{y}= &#92;arg&#92;min_y F(x,y)$&lt;/p&gt;
&lt;p&gt;Another advantage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By varying the latent variable over a set, we can make the prediction output $y$ vary over the manifold of possible predictions as well: $F(x,y) = &#92;arg&#92;min_zE(x,y,z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;
&lt;p&gt;Constraints:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P(x) &#92;geq 0 &#92; $ is non-negative&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$&#92;sum_x P(x) = 1 &#92; $ OR $&#92; &#92;int_x P(x) dx = 1 &#92; $ if continuous is normalized&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

			
		</content>
	</entry>
</feed>

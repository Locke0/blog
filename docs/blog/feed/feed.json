{
  "version": "https://jsonfeed.org/version/1",
  "title": "Yangyue&#39;s Blog",
  "home_page_url": "https://locke0.github.io/blog",
  "feed_url": "https://locke0.github.io/feed/feed.json",
  "description": "projects, ideas, solutions",
  "author": {
    "name": "Yangyue (Locke) Wang",
    "url": ""
  },
  "items": [{
      "id": "https://locke0.github.io/blog/posts/ebm_sn/",
      "url": "https://locke0.github.io/blog/posts/ebm_sn/",
      "title": "Composable Energy Based Models Study Notes",
      "content_html": "<h3 id=\"key-concepts\">Key Concepts</h3>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Energy Function</td>\n<td>A scalar function that assigns low values to likely configurations and high values to unlikely ones</td>\n</tr>\n<tr>\n<td>Partition Function</td>\n<td>Normalizing constant Z that ensures probability distribution sums/integrates to 1</td>\n</tr>\n<tr>\n<td>Gradient Descent</td>\n<td>Optimization method used to find local minima of energy function during inference</td>\n</tr>\n<tr>\n<td>MCMC</td>\n<td>Sampling method used to approximate expectations and generate samples from model</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"ebm-overview\">EBM Overview</h3>\n<p><mark>Energy based models</mark> predict compatibility between variable configurations rather than direct input-output mappings. The core idea is that energy functions assign low energy to likely configurations and high energy to unlikely ones.</p>\n<p>Key features:</p>\n<ul>\n<li><strong><em>Implicit Generation</em></strong>: Sample generation via MCMC from energy function</li>\n<li><strong><em>Compositionality</em></strong>: Multiple energy functions can be combined</li>\n<li><strong><em>Flexibility</em></strong>: Can model diverse probability distributions and data types</li>\n</ul>\n<h3 id=\"math-foundations\">Math Foundations</h3>\n<h4 id=\"energy-functions-and-probability\">Energy Functions and Probability</h4>\n<ol>\n<li>\n<p>Energy Function Mapping:</p>\n<ul>\n<li>Domain: Configurations in $\\mathcal{X} \\times \\mathcal{Y}$</li>\n<li>Range: Real numbers $\\mathbb{R}$</li>\n<li>Formal notation: $\\mathcal{F}: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$</li>\n</ul>\n</li>\n<li>\n<p>Prediction Process:</p>\n<ul>\n<li>Goal: Find minimum energy configurations</li>\n<li>Formula: $\\check{y} = \\arg\\min_y \\mathcal{F}(x,y)$</li>\n</ul>\n</li>\n<li>\n<p>Probability Connection:</p>\n<ul>\n<li>Via Boltzmann distribution</li>\n<li>Formula: $y \\sim p(x,y) \\propto e^{-E_{\\theta}(x,y)}$</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"training-objectives\">Training Objectives</h3>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Formula</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NLL Loss</td>\n<td>$L_NLL(D) = E_z~D[E_{\\theta}(z)] + \\log \\int e^{-E_{\\theta}(z)} dz$</td>\n</tr>\n<tr>\n<td>Gradient</td>\n<td>$\\nabla_{\\theta} L_NLL(D) = E_z~D[\\nabla_{\\theta} E_{\\theta}(z)] - E_z~p_{\\theta}(z)[\\nabla_{\\theta} E_{\\theta}(z)]$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"latent-variable-models\">Latent Variable Models</h3>\n<p>Step 1: Joint Optimization</p>\n<ul>\n<li>Variables: z (latent), x (input), y (output)</li>\n<li>Formula: $\\check{y}, \\check{z} = \\arg\\min_{y,z} E(x,y,z)$</li>\n</ul>\n<p>Step 2: Free Energy Formulation</p>\n<ul>\n<li>Definition: $F_{\\beta}(x,y) = -\\frac{1}{\\beta} \\log \\int_z \\exp(-\\beta E(x,y,z))$</li>\n</ul>\n<p>Step 3: Limiting Behavior ($\\beta \\rightarrow \\infty$)</p>\n<ul>\n<li>Energy: $F_{\\infty}(x,y) = \\arg\\min_z E(x,y,z)$</li>\n<li>Prediction: $\\check{y} = \\arg\\min_y F(x,y)$</li>\n</ul>\n<h3 id=\"training-and-sampling\">Training and Sampling</h3>\n<h4 id=\"maximum-likelihood-training\">Maximum Likelihood Training</h4>\n<p>MLE objective:</p>\n<ul>\n<li>$L_NLL(D) = E_z~D[E_{\\theta}(z)] - E_z~p_{\\theta}(z)[E_{\\theta}(z)]$</li>\n</ul>\n<h4 id=\"langevin-dynamics-sampling\">Langevin Dynamics Sampling</h4>\n<p>Update rule:</p>\n<ul>\n<li>$z_t = z_{t-1} - λ \\nabla_z E_{\\theta}(z_{t-1}) + \\sqrt{2λ} \\xi, \\xi \\sim N(0,1)$</li>\n</ul>\n<h4 id=\"practical-considerations\">Practical Considerations</h4>\n<ul>\n<li>Use replay buffers to improve sample diversity</li>\n<li>Apply regularization to smooth energy landscapes</li>\n<li>Consider annealed landscapes for high-dimensional problems</li>\n</ul>\n<h3 id=\"compositional-modeling\">Compositional Modeling</h3>\n<h4 id=\"model-composition\">Model Composition</h4>\n<ul>\n<li>Logical composition of energy landscapes</li>\n<li>Probability combinations via products and mixtures</li>\n<li>Hierarchical composition for complex tasks</li>\n</ul>\n<h3 id=\"applications\">Applications</h3>\n<ul>\n<li>Computer Vision: Scene understanding, image generation</li>\n<li>Robotics: Planning, constraint satisfaction</li>\n<li>Foundation Models: Vision-language tasks, hierarchical planning</li>\n<li>Scientific Applications: Inverse design, protein synthesis</li>\n</ul>\n<h3 id=\"future-directions\">Future Directions</h3>\n<ul>\n<li>Scale to more complex data distributions</li>\n<li>Develop improved sampling techniques</li>\n<li>Expand applications in sciences and engineering</li>\n<li>Enhance zero-shot generalization capabilities</li>\n</ul>\n",
      "date_published": "2024-11-11T19:00:00-05:00"
    }
  ]
}

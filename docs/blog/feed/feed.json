{
  "version": "https://jsonfeed.org/version/1",
  "title": "Yangyue&#39;s Blog",
  "home_page_url": "https://locke0.github.io/blog",
  "feed_url": "https://locke0.github.io/feed/feed.json",
  "description": "projects, ideas, solutions",
  "author": {
    "name": "Yangyue (Locke) Wang",
    "url": ""
  },
  "items": [{
      "id": "https://locke0.github.io/blog/posts/why_do_computer_use_agents_fail/",
      "url": "https://locke0.github.io/blog/posts/why_do_computer_use_agents_fail/",
      "title": "Why Do Computer Use Agents Fail? The Brittle Stack of AI Limitations",
      "content_html": "<h3 id=\"0-introduction\">0. Introduction</h3>\n<p>The promise of computer use agents (CUAs), AI systems that can perceive and act on a computer screen just like a human, is a compelling one. We envision an intelligent assistant that can:</p>\n<ul>\n<li>Book a flight, hotel, &amp; activities</li>\n<li>Organize a annual budget planning spreadsheet</li>\n<li>Debug robotics CAD design</li>\n<li>Navigate complex game engine interfaces</li>\n</ul>\n<p><img src=\"/blog/img/why_do_computer_use_agents_fail/ui-tars-tutorial-screenshot.png\" alt=\"Human user following tutorial instructions to select spreadsheet data\"><br>\n(<em><a href=\"https://seed-tars.com/1.5\">UI-TARS research, 2025</a></em>)</p>\n<p>Over the past year, we've seen remarkable progress, with models like UI-TARS-1.5, OpenCUA, &amp; Claude Sonnet 4, and multi-agent systems like CoACT, GTA-1 w/ o3, JEDI w/ o3, &amp; Agent S2 setting new performance records on benchmarks like OSWorld and ScreenSpot-Pro. Yet, despite these advances, the general-purpose CUA remains elusive.</p>\n<hr>\n<h3 id=\"1-the-perception-challenge-the-world-is-not-always-what-it-seems\">1. The Perception Challenge: The World Is Not Always What It Seems</h3>\n<p>The first and most fundamental challenge for any CUA is perception—the ability to interpret the visual information on a screen. Unlike a human, an agent doesn't have an intuitive understanding of a graphical user interface (GUI). It just sees pixels, and its job is to make sense of them.</p>\n<p><strong>Visual Placeholder: [Side-by-side comparison: Human sees &quot;Submit button&quot; vs. AI sees &quot;pixel array with RGB values&quot;]</strong></p>\n<h4 id=\"11-the-problem-of-tail-classes\">1.1 The Problem of Tail Classes</h4>\n<p><strong>What it is:</strong> Most agents are trained on common websites and popular applications, creating a distribution imbalance.</p>\n<p><strong>The Problem:</strong></p>\n<ul>\n<li>✅ <strong>Excel at recognizing:</strong> Standard Submit buttons, common UI elements</li>\n<li>❌ <strong>Completely lost with:</strong> Rare or specialized UI components</li>\n</ul>\n<p><strong>Examples of problematic &quot;tail classes&quot;:</strong></p>\n<ul>\n<li>Custom data visualization widgets in scientific apps</li>\n<li>Niche financial charting tools</li>\n<li>Complex, multi-layered data selectors</li>\n<li>Highly-specific industrial control panel elements</li>\n<li>Obscure icons from legacy applications</li>\n</ul>\n<p><strong>Why it happens:</strong> The model hasn't seen enough examples of these elements in various contexts to build a robust mental model.</p>\n<p><strong>Visual Placeholder: [Venn diagram showing &quot;Common UI Elements&quot; vs. &quot;Tail Class Elements&quot; with training data distribution]</strong></p>\n<h4 id=\"12-small-targets-in-a-big-world\">1.2 Small Targets in a Big World</h4>\n<p><strong>The Challenge:</strong> Modern screens have astonishingly high resolution, making critical elements tiny targets.</p>\n<p><strong>Problems:</strong></p>\n<ul>\n<li>Critical buttons occupy only a tiny fraction of the screen</li>\n<li>State-of-the-art models often fail on benchmarks like ScreenSpot-Pro</li>\n<li>Vision encoders may be trained at lower resolution</li>\n<li>Elements might be partially occluded by pop-ups or sidebars</li>\n</ul>\n<p><strong>Visual Placeholder: [Screenshot of a high-resolution screen with tiny UI elements highlighted and zoomed in]</strong></p>\n<h4 id=\"13-ambiguous-semantics\">1.3 Ambiguous Semantics</h4>\n<p><strong>The Problem:</strong> Icons and visual elements are not always universal.</p>\n<p><strong>Examples of ambiguity:</strong></p>\n<ul>\n<li>Cloud icon: Does it mean &quot;upload to cloud,&quot; &quot;show weather,&quot; or &quot;link to a cloud document&quot;?</li>\n<li>Lightning bolt: &quot;Cast a spell&quot; in a game vs. &quot;express route&quot; in a transportation app</li>\n</ul>\n<p><strong>Root Cause:</strong> Absence of clear, unambiguous textual labels leads to critical misinterpretations.</p>\n<p><strong>Visual Placeholder: [Grid of ambiguous icons with multiple possible meanings shown]</strong></p>\n<hr>\n<h3 id=\"2-the-reasoning-challenge-from-task-to-plan-to-action\">2. The Reasoning Challenge: From Task to Plan to Action</h3>\n<p>Once an agent can perceive the screen, it must reason about the task and form a plan. This involves connecting high-level instructions to low-level actions.</p>\n<p><strong>Visual Placeholder: [Flowchart: &quot;Find cheapest flight&quot; → &quot;Click search box&quot; → &quot;Type destination&quot; → &quot;Click search button&quot;]</strong></p>\n<h4 id=\"21-the-hallucinating-planner\">2.1 The Hallucinating Planner</h4>\n<p><strong>What happens:</strong> The agent generates plausible-sounding plans involving elements that don't exist.</p>\n<p><strong>Root Causes:</strong></p>\n<ul>\n<li>Limited context window prevents retaining full UI mental model</li>\n<li>Agent &quot;remembers&quot; elements from previous screenshots no longer present</li>\n<li>Chain-of-Thought (CoT) becomes disconnected from visual reality</li>\n</ul>\n<p><strong>Analogy:</strong> Like a person with short-term memory loss trying to follow a complex recipe.</p>\n<p><strong>Visual Placeholder: [Timeline showing agent's memory degradation and hallucination of non-existent UI elements]</strong></p>\n<h4 id=\"22-the-symbol-grounding-gap\">2.2 The Symbol Grounding Gap</h4>\n<p><strong>The Problem:</strong> Agent knows the symbol but not the underlying function.</p>\n<p><strong>Example:</strong> Agent sees &quot;delete&quot; and &quot;remove&quot; buttons but doesn't understand:</p>\n<ul>\n<li>One is permanent</li>\n<li>One is reversible</li>\n</ul>\n<p><strong>Current State:</strong> Agent's reasoning is purely correlational, not causal.</p>\n<p><strong>Research Direction:</strong> Frameworks that explicitly model UI state transitions (e.g., UI-TARS).</p>\n<p><strong>Visual Placeholder: [State transition diagram showing before/after states for different button actions]</strong></p>\n<h3 id=\"23-task-deviation\">2.3 Task Deviation</h3>\n<p><strong>What happens:</strong> Agent gets sidetracked from the original goal.</p>\n<p><strong>Example Task:</strong> &quot;Organize my downloads folder&quot;</p>\n<ul>\n<li>✅ Successfully finds the folder</li>\n<li>❌ Gets sidetracked by &quot;open every PDF&quot;</li>\n<li>❌ Never completes the original task</li>\n</ul>\n<p><strong>Solution:</strong> Modular, hierarchical frameworks like Agent S2 use a &quot;Manager&quot; agent to keep &quot;Worker&quot; agents on track.</p>\n<p><strong>Visual Placeholder: [Hierarchical task tree showing main goal and sub-goals with deviation paths highlighted]</strong></p>\n<hr>\n<h3 id=\"3-the-action-challenge-the-friction-of-the-real-world\">3. The Action Challenge: The Friction of the Real World</h3>\n<p>Even with perfect perception and flawless planning, execution remains challenging in a dynamic, unpredictable environment.</p>\n<p><strong>Visual Placeholder: [Diagram showing the gap between planned actions and real-world execution challenges]</strong></p>\n<h4 id=\"31-the-brittle-action-space\">3.1 The Brittle Action Space</h4>\n<p><strong>Current Limitation:</strong> Most agents are limited to fixed atomic actions (e.g., <code>click(x, y)</code>).</p>\n<p><strong>What's Missing:</strong></p>\n<ul>\n<li>Complex drag-and-drop operations</li>\n<li>Multi-finger touch gestures</li>\n<li>Precise slider movements</li>\n<li>Programmatic control</li>\n</ul>\n<p><strong>Emerging Solution:</strong> Agents generating code (e.g., Python scripts) to control the environment directly.</p>\n<p><strong>Visual Placeholder: [Comparison table: Current actions vs. needed actions vs. future capabilities]</strong></p>\n<h4 id=\"32-lack-of-fault-tolerance\">3.2 Lack of Fault Tolerance</h4>\n<p><strong>The Problem:</strong> What happens when things go wrong?</p>\n<p><strong>Failure Scenarios:</strong></p>\n<ul>\n<li>Website unresponsive</li>\n<li>Element fails to load</li>\n<li>Action doesn't produce expected result</li>\n<li>HTTP 500 errors</li>\n<li>No response from commands</li>\n</ul>\n<p><strong>Current State:</strong> Agent gets stuck or repeats failing actions.</p>\n<p><strong>Research Direction:</strong> Reflection and error-correction loops for robust recovery.</p>\n<p><strong>Visual Placeholder: [Error handling flowchart showing detection → diagnosis → recovery → retry]</strong></p>\n<h4 id=\"33-memory-and-state-drift\">3.3 Memory and State Drift</h4>\n<p><strong>The Challenge:</strong> Agent's internal state must stay synchronized with dynamic screen state.</p>\n<p><strong>Problems:</strong></p>\n<ul>\n<li>Operating on outdated UI information</li>\n<li>Missing animations, pop-ups, or transient changes</li>\n<li>Stale mental model of the interface</li>\n</ul>\n<p><strong>Analogy:</strong> Difference between a static photograph and a live video stream.</p>\n<p><strong>Solutions:</strong> Sophisticated, persistent memory systems (Zep AI, Mem0).</p>\n<p><strong>Visual Placeholder: [Timeline showing state drift over time with before/after screenshots]</strong></p>\n<hr>\n<h3 id=\"4-the-road-ahead-solutions-and-research-directions\">4. The Road Ahead: Solutions and Research Directions</h3>\n<p>The failures of computer use agents are not fundamental flaws but a clear roadmap for future research.</p>\n<p><strong>Visual Placeholder: [Roadmap diagram showing current state → challenges → solutions → future vision]</strong></p>\n<h4 id=\"41-toward-end-to-end-models\">4.1 Toward End-to-End Models</h4>\n<p><strong>Current Approach:</strong> Modular, separate components for perception, reasoning, and action.</p>\n<p><strong>Future Direction:</strong> Training agents on huge, multi-modal datasets capturing full human workflow.</p>\n<p><strong>Benefits:</strong></p>\n<ul>\n<li>Learn causal relationships between actions and outcomes</li>\n<li>Reduce symbol grounding gaps</li>\n<li>Integrated learning across all components</li>\n</ul>\n<p><strong>Inspiration:</strong> Success of large foundation models in other domains.</p>\n<p><strong>Visual Placeholder: [Architecture comparison: Current modular vs. future end-to-end]</strong></p>\n<h4 id=\"42-better-benchmarks\">4.2 Better Benchmarks</h4>\n<p><strong>Current State:</strong> Simple, canned tasks in controlled environments.</p>\n<p><strong>Future Direction:</strong> Live, interactive benchmarks like BrowseComp and WebArena.</p>\n<p><strong>Why It Matters:</strong> Introduces real-world variability:</p>\n<ul>\n<li>Fluctuating ad placements</li>\n<li>Session timeouts</li>\n<li>Different website versions</li>\n<li>Dynamic content changes</li>\n</ul>\n<p><strong>Visual Placeholder: [Benchmark comparison chart showing complexity levels and real-world factors]</strong></p>\n<h4 id=\"43-the-power-of-self-improvement\">4.3 The Power of Self-Improvement</h4>\n<p><strong>Key Trend:</strong> Agents learning from their own failures.</p>\n<p><strong>Techniques:</strong></p>\n<ul>\n<li>Online trace bootstrapping</li>\n<li>Direct preference optimization (DPO)</li>\n<li>Agent-in-the-loop processes</li>\n</ul>\n<p><strong>Process:</strong></p>\n<ol>\n<li>Agent performs task</li>\n<li>Performance evaluated</li>\n<li>Feedback used to correct mistakes</li>\n<li>Reasoning refined for future attempts</li>\n</ol>\n<p><strong>Visual Placeholder: [Learning loop diagram showing performance → evaluation → correction → improvement]</strong></p>\n<h4 id=\"44-beyond-the-pixels\">4.4 Beyond the Pixels</h4>\n<p><strong>Current Limitation:</strong> Agents only look at visual pixels.</p>\n<p><strong>Future Enhancement:</strong> Leverage multiple information sources:</p>\n<ul>\n<li>HTML DOM tree</li>\n<li>Accessibility APIs</li>\n<li>System logs</li>\n<li>Underlying metadata</li>\n</ul>\n<p><strong>Example:</strong> DOM tree tells agent <code>&lt;button id=&quot;submit&quot;&gt;</code> rather than just visual appearance.</p>\n<p><strong>Benefit:</strong> More robust to visual changes and layout shifts.</p>\n<p><strong>Visual Placeholder: [Multi-modal information fusion diagram showing pixels + DOM + APIs + logs]</strong></p>\n<hr>\n<h3 id=\"5-conclusion\">5. Conclusion</h3>\n<p>The journey to a truly generalist computer use agent is ongoing. With each new paper and each failed run on a leaderboard, we get:</p>\n<ul>\n<li>A clearer picture of the challenges ahead</li>\n<li>A better sense of how to solve them</li>\n<li>Progress toward robust, real-world AI assistants</li>\n</ul>\n<p><strong>Visual Placeholder: [Progress timeline showing key milestones and future goals]</strong></p>\n<p><strong>Key Takeaway:</strong> The failures we're seeing today are not setbacks but stepping stones toward more capable, robust computer use agents that can truly understand and navigate the digital world as humans do.</p>\n",
      "date_published": "2025-08-21T06:26:30Z"
    }
  ]
}

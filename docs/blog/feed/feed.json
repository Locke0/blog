{
  "version": "https://jsonfeed.org/version/1",
  "title": "Yangyue&#39;s Blog",
  "home_page_url": "https://locke0.github.io/blog",
  "feed_url": "https://locke0.github.io/feed/feed.json",
  "description": "projects, ideas, solutions",
  "author": {
    "name": "Yangyue (Locke) Wang",
    "url": ""
  },
  "items": [{
      "id": "https://locke0.github.io/blog/posts/ebm_sn/",
      "url": "https://locke0.github.io/blog/posts/ebm_sn/",
      "title": "Composable Energy Based Models Study Notes",
      "content_html": "<h2 id=\"key-concepts\">Key Concepts</h2>\n<ul>\n<li>Energy Functions</li>\n<li>Partition Functions</li>\n<li>Gradient Descent and Optimization</li>\n<li>Markov Chain Monte Carlo</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Energy Function</td>\n<td>A scalar function that assigns low values to likely configurations and high values to unlikely ones</td>\n</tr>\n<tr>\n<td>Partition Function</td>\n<td>Normalizing constant Z that ensures probability distribution sums/integrates to 1</td>\n</tr>\n<tr>\n<td>Gradient Descent</td>\n<td>Optimization method used to find local minima of energy function during inference</td>\n</tr>\n<tr>\n<td>MCMC</td>\n<td>Sampling method used to approximate expectations and generate samples from model</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"overview\">Overview</h2>\n<p>Instead of regressing input x to output y, <mark>energy based models</mark> predict whether a certain pair or configuration of variables fit together. Energy functions assign low energy to likely configurations and high energy to unlikely ones.</p>\n<h2 id=\"definitions\">Definitions</h2>\n<p>$\\mathcal{F}: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$</p>\n<ul>\n<li>$\\mathcal{F}(x,y)$: describes the level of dependency between x and y</li>\n</ul>\n<p>Inference:</p>\n<ul>\n<li>$\\check{y} = \\arg\\min_y \\mathcal{F}(x,y)$</li>\n</ul>\n<p>Energy function (used in inference not training):</p>\n<ul>\n<li>It needs to be smooth and differentiable to perform gradient-based method for inference</li>\n</ul>\n<p>SIDE NOTE: Graphical models' energy function decomposes as <em>a sum of energy terms</em>, each of which accounts a subset of variables.</p>\n<p>EBM with latent variables</p>\n<p>y depends on x as well as an extra variable z (the latent variable).<br>\nThese latent variables can provide auxiliary information.</p>\n<h3 id=\"inference\">Inference</h3>\n<p>Minimize the energy function simulataneously over y and z:</p>\n<ul>\n<li>$\\check{y}, \\check{z} = \\arg\\min_{y,z} E(x,y,z)$</li>\n</ul>\n<p>which is equivalent to:</p>\n<p>$\\Rightarrow F_{\\infty}(x,y) = \\arg\\min_z E(x,y,z)$</p>\n<p>$\\Rightarrow F_{\\beta}(x,y) = -\\frac{1}{\\beta} \\log \\int_z \\exp(-\\beta E(x,y,z))$</p>\n<p>when $\\ \\beta \\rightarrow \\infty$, $\\ \\check{y}= \\arg\\min_y F(x,y)$</p>\n<p>Another advantage:</p>\n<ul>\n<li>By varying the latent variable over a set, we can make the prediction output $y$ vary over the manifold of possible predictions as well: $F(x,y) = \\arg\\min_zE(x,y,z)$</li>\n</ul>\n<h3 id=\"training\">Training</h3>\n<p>Constraints:</p>\n<ol>\n<li>\n<p>$P(x) \\geq 0 \\ $ is non-negative</p>\n</li>\n<li>\n<p>$\\sum_x P(x) = 1 \\ $ OR $\\ \\int_x P(x) dx = 1 \\ $ if continuous is normalized</p>\n</li>\n</ol>\n<h2 id=\"applications\">Applications</h2>\n<ol>\n<li>Video prediction</li>\n</ol>\n<h2 id=\"energy-based-models-(ebms)\">Energy-Based Models (EBMs)</h2>\n<p>Energy-Based Models (EBMs) are a class of machine learning models that learn an energy function E(x) to assign low energy to inputs from the data distribution and high energy to others. Key features include:</p>\n<ul>\n<li><strong><em>Implicit Generation</em></strong>: Samples are generated implicitly through MCMC sampling from the energy function, rather than explicitly by a generator network</li>\n<li><strong><em>Compositionality</em></strong>: EBMs can combine multiple energy functions to satisfy multiple constraints or goals</li>\n<li><strong><em>Flexibility</em></strong>: EBMs can represent a wide range of probability distributions and can be applied to various types of data</li>\n</ul>\n<h2 id=\"composable-energy-landscapes\">Composable Energy Landscapes</h2>\n<p>Composable Energy Landscapes provide a framework for constructing models that generalize by composing multiple energy landscapes. This approach allows for:</p>\n<ul>\n<li><strong>Zero-Shot Generalization</strong>: The ability to generalize to new distributions by composing learned energy landscapes without additional training</li>\n<li><strong>Energy Function E_θ(x, y)</strong>: Assigns low energy to accurate predictions, enabling prediction as a search process on the energy landscape</li>\n</ul>\n<h2 id=\"mathematical-foundations\">Mathematical Foundations</h2>\n<h3 id=\"energy-function-and-boltzmann-distribution\">Energy Function and Boltzmann Distribution</h3>\n<p>The energy function is central to EBMs, with prediction formulated as finding the label y that minimizes the energy:</p>\n<p>y = argmin_y E_θ(x,y)</p>\n<p>The Boltzmann distribution is used for sampling predictions:</p>\n<p>y ~ p(x,y) ∝ e^(-E_θ(x,y))</p>\n<h3 id=\"training-objectives\">Training Objectives</h3>\n<p>The primary training objective for EBMs is the Negative Log-Likelihood (NLL):</p>\n<p>L_NLL(D) = E_z~D[E_θ(z)] + log∫e^(-E_θ(z))dz</p>\n<p>The gradient of the NLL is given by:</p>\n<p>∇_θL_NLL(D) = E_z~D[∇_θE_θ(z)] - E_z~p_θ(z)[∇_θE_θ(z)]</p>\n<h2 id=\"training-and-sampling-techniques\">Training and Sampling Techniques</h2>\n<h3 id=\"maximum-likelihood-estimation-(mle)\">Maximum Likelihood Estimation (MLE)</h3>\n<p>MLE is used to minimize the negative log-likelihood:</p>\n<p>L_NLL(D) = E_z~D[E_θ(z)] - E_z~p_θ(z)[E_θ(z)]</p>\n<h3 id=\"langevin-dynamics\">Langevin Dynamics</h3>\n<p>Langevin dynamics is a gradient-based MCMC method used for sampling:</p>\n<p>z_t = z_(t-1) - λ∇_zE_θ(z_(t-1)) + √(2λ)ξ, ξ ~ N(0,1)</p>\n<h3 id=\"replay-buffer\">Replay Buffer</h3>\n<p>A replay buffer is used to maintain past samples, improving mixing and sample diversity.</p>\n<h2 id=\"compositional-modeling\">Compositional Modeling</h2>\n<h3 id=\"logical-and-probability-composition\">Logical and Probability Composition</h3>\n<p>EBMs allow for logical composition of energy landscapes to model complex distributions. Probability distributions can be combined through products, mixtures, and inverses.</p>\n<h3 id=\"graphical-and-hierarchical-models\">Graphical and Hierarchical Models</h3>\n<p>EBMs can implement undirected and directed graphical models compositionally. They also enable hierarchical composition, combining language, video, and action models for complex planning tasks.</p>\n<h2 id=\"applications-2\">Applications</h2>\n<p>EBMs and composable energy landscapes have diverse applications:</p>\n<ul>\n<li><strong>Vision</strong>: Scene understanding, image generation and editing, domain adaptation</li>\n<li><strong>Robotics</strong>: Planning and constraint satisfaction problems</li>\n<li><strong>Foundation Models</strong>: Vision-question answering and hierarchical planning</li>\n</ul>\n<h2 id=\"observations-and-experiment-results\">Observations and Experiment Results</h2>\n<ul>\n<li><strong>Generalization</strong>: Compositional models show strong generalization to unseen data distributions</li>\n<li><strong>Efficiency</strong>: These models often require fewer parameters and less data</li>\n<li><strong>Flexibility</strong>: They allow for the incorporation of new constraints at prediction time</li>\n</ul>\n<h2 id=\"tutorials-and-practical-tips\">Tutorials and Practical Tips</h2>\n<ul>\n<li>Apply regularization to ensure energy landscapes are not too sharp</li>\n<li>Learn sequences of energy landscapes for high-dimensional inputs (annealed energy landscapes)</li>\n</ul>\n<h2 id=\"future-directions\">Future Directions</h2>\n<ul>\n<li><strong>Inverse Design</strong>: Apply composable energy landscapes to inverse material design and protein synthesis</li>\n<li><strong>Broader Applications</strong>: Explore applications in sciences and engineering</li>\n<li><strong>Scalability</strong>: Develop techniques to scale energy landscape learning to more complex and high-dimensional data</li>\n</ul>\n",
      "date_published": "2024-11-11T19:00:00-05:00"
    }
  ]
}

{
  "version": "https://jsonfeed.org/version/1",
  "title": "Yangyue&#39;s Blog",
  "home_page_url": "https://locke0.github.io/blog",
  "feed_url": "https://locke0.github.io/feed/feed.json",
  "description": "projects, ideas, solutions",
  "author": {
    "name": "Yangyue (Locke) Wang",
    "url": ""
  },
  "items": [{
      "id": "https://locke0.github.io/blog/posts/ebm_sn/",
      "url": "https://locke0.github.io/blog/posts/ebm_sn/",
      "title": "Composable Energy Based Models Study Notes",
      "content_html": "<h2 id=\"key-concepts\">Key Concepts</h2>\n<ul>\n<li>Energy Functions</li>\n<li>Partition Functions</li>\n<li>Gradient Descent and Optimization</li>\n<li>Markov Chain Monte Carlo</li>\n</ul>\n<h2 id=\"overview\">Overview</h2>\n<p>Instead of regressing input x to output y, energy based models predict whether a certain pair or configuration of variables fit together. Energy functions assign low energy to likely configurations and high energy to unlikely ones.</p>\n<h2 id=\"definitions\">Definitions</h2>\n<p>$\\mathcal{F}: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$</p>\n<ul>\n<li>$\\mathcal{F}(x,y)$: describes the level of dependency between x and y</li>\n</ul>\n<p>Inference:</p>\n<ul>\n<li>$\\check{y} = \\arg\\min_y \\mathcal{F}(x,y)$</li>\n</ul>\n<p>Energy function (used in inference not training):</p>\n<ul>\n<li>It needs to be smooth and differentiable to perform gradient-based method for inference</li>\n</ul>\n<p>SIDE NOTE: Graphical models' energy function decomposes as <em>a sum of energy terms</em>, each of which accounts a subset of variables.</p>\n<p>EBM with latent variables</p>\n<p>y depends on x as well as an extra variable z (the latent variable).<br>\nThese latent variables can provide auxiliary information.</p>\n<h3 id=\"inference\">Inference</h3>\n<p>Minimize the energy function simulataneously over y and z:</p>\n<ul>\n<li>$\\check{y}, \\check{z} = \\arg\\min_{y,z} E(x,y,z)$</li>\n</ul>\n<p>which is equivalent to:</p>\n<p>$\\Rightarrow F_{\\infty}(x,y) = \\arg\\min_z E(x,y,z)$</p>\n<p>$\\Rightarrow F_{\\beta}(x,y) = -\\frac{1}{\\beta} \\log \\int_z \\exp(-\\beta E(x,y,z))$</p>\n<p>when $\\ \\beta \\rightarrow \\infty$, $\\ \\check{y}= \\arg\\min_y F(x,y)$</p>\n<p>Another advantage:</p>\n<ul>\n<li>By varying the latent variable over a set, we can make the prediction output $y$ vary over the manifold of possible predictions as well: $F(x,y) = \\arg\\min_zE(x,y,z)$</li>\n</ul>\n<h3 id=\"training\">Training</h3>\n<p>Constraints:</p>\n<ol>\n<li>\n<p>$P(x) \\geq 0 \\ $ is non-negative</p>\n</li>\n<li>\n<p>$\\sum_x P(x) = 1 \\ $ OR $\\ \\int_x P(x) dx = 1 \\ $ if continuous is normalized</p>\n</li>\n</ol>\n<h2 id=\"applications\">Applications</h2>\n<ol>\n<li>Video prediction</li>\n</ol>\n",
      "date_published": "2024-11-11T19:00:00-05:00"
    }
  ]
}

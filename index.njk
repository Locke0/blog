---
title: ...
layout: layouts/home.njk
description:
image: img/remote/solarpunk_mountains.jpg
headerImagePosition: center 60%
date: Last Modified
---

<div class="professional-statement">

      <div class="profile-section">
        <img src="img/remote/profile-image.jpg" class="profile-image">
      </div>
      
      <p>Hello! Welcome to my thinking dojo where I document my progress on teaching rocks to think. I hope you find something that makes you curious.</p>

      <p>
        <a href="mailto:yangyuewang123@gmail.com">Email</a> |
        <a href="https://github.com/Locke0">GitHub</a> |
        <a href="https://scholar.google.com/citations?hl=en&user=oWaNDDEAAAAJ&view_op=list_works&gmla=AH8HC4xS-yisEugoAtoMYNQbpmpDFC1GNwOXLBK60JpgiiWtAoYF77LYmbYcQMpKp5xTKrnn8uHxPLV2mYP4-2k9">Google Scholar</a> |
        <a href="https://x.com/Yangyue_Wang">Twitter</a> |
        <a href="https://www.linkedin.com/in/yangyue-wang/">LinkedIn</a>
      </p>

      <hr>

      <!---<div id="posts">
        <h3>Blog</h3>
        {% set postslist = collections.posts %}
        {% if postslist | length > 0 %}
          {% include "postslist.njk" %}
        {% endif %}
      <hr>---!>

      <div id="publications">
        <h3>Publications</h3>
        <ul>
          <li><a href="https://multinet.ai">An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models</a> - Jun 2025<br>[<a href="https://multinet.ai/">project</a>] [<a href="https://arxiv.org/abs/2506.09172">paper</a>]
          </li>
          <li><a href="https://multinet.ai/static/pages/Multinetv02.html">Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments</a> - Jun 2025<br>[<a href="https://multinet.ai/">project</a>] [<a href="https://arxiv.org/abs/2505.05540">paper</a>] [<a href="https://github.com/ManifoldRG/MultiNet?tab=readme-ov-file">code</a>]
          </li>
          <li><a href="https://multinet.ai/static/pages/Multinetv01.html">Benchmarking Vision, Language, & Action Models On Robotic Learning Tasks</a> - Nov 2024<br>[<a href="https://multinet.ai/">project</a>] [<a href="https://arxiv.org/abs/2411.05821">paper</a>] [<a href="https://github.com/ManifoldRG/MultiNet?tab=readme-ov-file">code</a>]
          </li>
        </ul>
      </div>

      <hr>

      <div id="projects">
        <h3>Projects</h3>
        <ul>
          <li><strong>MultiNet:</strong> A benchmark for evaluating vision, language, and action models on a suite of robotic learning tasks - 2025 <br>[<a href="https://multinet.ai/">Project</a>] [<a href="https://github.com/ManifoldRG/MultiNet?tab=readme-ov-file">Code</a>]</li>
          <li><strong>Software Control Agents:</strong> A project focused on developing agents that can reliably control and interact with diverse software applications - 2025 <br>[<a href="https://github.com/ManifoldRG/software-control">Code</a>]</li>
        </ul>
      </div>

    </div>

